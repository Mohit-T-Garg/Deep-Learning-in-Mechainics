# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FkO87nxKMJwQ_i3OOrxcExU9dT-rtoo0

**Import Required Libraries**
"""

import torch     
from torch import Tensor                  
import torch.nn as nn                 
import torch.optim as optim              
import matplotlib.pyplot as plt
import numpy as np
import itertools
from collections import OrderedDict

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)

if device == 'cuda': 
    print(torch.cuda.get_device_name())



"""Load the training data from numpy"""

d = np.load("antiderivative_aligned_train.npz", allow_pickle=True)
y_train=d["X"][1].astype(np.float32) #output locations (100,1)
u_train = d["X"][0].astype(np.float32) # input functions (150,100)
s_train = d["y"].astype(np.float32) # output functions (150,100)

"""**Define Network Archietecture**"""

# the deep neural network
class DNN(torch.nn.Module):
    def __init__(self, layers):
        #define your sequential model here with activations 
        super(DNN, self).__init__()
        self.layers = nn.Sequential()
        for i in range(1, len(layers)):
            self.layers.add_module(f'linear_{i}', nn.Linear(layers[i-1], layers[i]))
            self.layers.add_module(f'Tanh_{i}', nn.Tanh())
        print(self.layers)    
                
                
    def forward(self, x): #forward pass
        out = self.layers(x)
        return out   
"""**The DeepONet Archietecture**"""
class PI_DeepONet():
    def __init__(self, branch_layers, trunk_layers,u_train, y_train, s_train):
        
        self.u_train = torch.Tensor(u_train).to(device)  # Convert u_train to torch tensor
        self.y_train = torch.Tensor(y_train).to(device)  # Convert y_train to torch tensor
        self.s_train = torch.Tensor(s_train).to(device)  # Convert s_train to torch tensor
        
        
        self.branch_net = DNN(branch_layers).to(device)  # The branch Network

        self.trunk_net = DNN(trunk_layers).to(device)    # The trunk Network
        
        branch_params = list(self.branch_net.parameters())               #extract the network Parameters in list format
        trunk_params =   list(self.trunk_net.parameters()) 
        
        #extract the network Parameters
        params = branch_params+trunk_params
        
        self.optimizer_Adam = torch.optim.Adam(params, lr=0.01)

        self.iter = 0    #initiate iteration

    
    def operator_net(self,  u, y):    # Define DeepONet architecture
        
        B = self.branch_net(u) #output from branch Network
        T = self.trunk_net(y)  #output from branch Network 
        output = torch.matmul(B, torch.transpose(T,0,1))
        return output
        
    # Define operator loss
    def loss_operator(self, u, y):
        
        pred = self.operator_net(u, y)         # Compute forward pass     
        loss =  torch.mean((self.s_train-pred)**2)        # Compute loss
        return loss

    
    def train(self, nIter):
        
        model_loss=np.array([])
        for epoch in range(nIter):
            loss= self.loss_operator(self.u_train,self.y_train)
             # Backward and optimize
           
            self.optimizer_Adam.zero_grad()
            loss.backward(retain_graph=True) 
            self.optimizer_Adam.step()

            model_loss=np.append(model_loss,loss.detach().numpy())   #get the loss value for each iteration

            if epoch % 10 == 0:
                print(f'Epoch [{epoch}/{nIter}], Loss: {loss.item()}')# print loss and iteration
        
        return model_loss 
             
    # Evaluates predictions at test points  
    def predict_s(self, u_star,y_star):
        u_star = torch.Tensor(u_star).to(device)
        y_star = torch.Tensor(y_star).to(device)
        s = self.operator_net(u_star, y_star)  # Predict
        s = s.detach().cpu().numpy()
        return s

"""**Train the Model**"""

# Initialize model

branch_layers = [100, 50, 50, 50, 50, 50]
trunk_layers =  [1, 50, 50, 50, 50, 50]

model = PI_DeepONet(branch_layers, trunk_layers,u_train, y_train, s_train)

 
'Neural Network Summary'
print(model)
nIter=2000
loss=model.train(nIter)


s_pred_train = model.predict_s(u_train,y_train)
x = np.linspace(0, 1, 100)
fig = plt.figure(figsize=(15,15))

for i in range(30):  
    r = np.random.choice(100)
    ax = fig.add_subplot(6, 5, i+1)
    ax.plot(x,s_pred_train[r],c = 'black', marker='.',label="Predicted")  
    ax.plot(x,s_train[r], c='r',label="Actual")
    plt.legend()
    plt.axis('off')


"""**Load Test Data**"""

d = np.load("antiderivative_aligned_test.npz", allow_pickle=True) 
B=d["X"][0].astype(np.float32)
u_test = d["X"][0].astype(np.float32); y_test=d["X"][1].astype(np.float32)
s_test = d["y"].astype(np.float32)

#Model Predictions 
s_pred = model.predict_s(u_test, y_test)


# Plot visualization
plt.figure(figsize=(8, 6))
plt.plot(loss)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Cost History')
plt.show()

  

fig1 = plt.figure(figsize=(15,15))
for i in range(30):  
    r = np.random.choice(1000)
    ax = fig1.add_subplot(6, 5, i+1)
    ax.plot(x,s_pred[r] , c = 'black', marker='.',label="predicted")  
    ax.plot(x,s_test[r] ,c='r',label="actual")
    plt.legend()
    plt.axis('off')    